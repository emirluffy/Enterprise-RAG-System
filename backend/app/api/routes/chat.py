"""
Enterprise RAG System - Real Google Gemini Chat API with RAG Pipeline (Context7 Verified)
"""
import asyncio
import time
import uuid
import re
from datetime import datetime
from fastapi import APIRouter, HTTPException, status, Depends, Request
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
from sqlmodel import Session, select
import re

# Request/Response Models (Context7 verified Pydantic v2 patterns)
class ChatQueryRequest(BaseModel):
    question: str = Field(..., description="User question")
    conversation_id: Optional[str] = Field(default="default", description="Conversation ID")
    # Multi-turn conversation support
    conversation_context: Optional[List[Dict[str, Any]]] = Field(default=[], description="Previous conversation messages")
    max_context_messages: int = Field(default=5, ge=1, le=20, description="Max previous messages to include")
    include_context: bool = Field(default=True, description="Whether to use conversation context")

class SourceInfo(BaseModel):
    source: str
    page: str | int
    score: float
    chunk_id: str
    # Enhanced fields (Context7 verified)
    slide_number: Optional[str] = Field(None, description="Specific slide number if available")
    relevance_percentage: Optional[float] = Field(None, description="Relevance score as percentage")
    content_type: Optional[str] = Field(None, description="Type of content (document, learned, etc.)")

class ProcessingMetrics(BaseModel):
    """Context7 verified metrics model"""
    response_time_ms: int = Field(..., description="Response time in milliseconds")
    confidence_score: float = Field(..., ge=0.0, le=1.0, description="Overall confidence score")
    documents_found: int = Field(..., ge=0, description="Number of documents found")
    learned_knowledge_found: int = Field(default=0, ge=0, description="Number of learned items found")
    embedding_dimension: int = Field(..., gt=0, description="Embedding vector dimension")
    search_strategy: str = Field(..., description="Search strategy used (hybrid/semantic/fallback)")

class StructuredSection(BaseModel):
    """Context7 verified structured content section"""
    title: str = Field(..., description="Section title")
    content: str = Field(..., description="Section content")
    icon: Optional[str] = Field(None, description="Unicode icon for the section")
    priority: int = Field(default=1, ge=1, le=10, description="Display priority (1=highest)")

class EnhancedChatResponse(BaseModel):
    """Context7 verified enhanced response model"""
    # Core response
    answer: str = Field(..., description="Main answer content")
    sections: List[StructuredSection] = Field(default_factory=list, description="Structured content sections")
    
    # Enhanced source information
    sources: List[SourceInfo] = Field(default_factory=list, description="Enhanced source citations")
    
    # Detailed metrics
    metrics: ProcessingMetrics = Field(..., description="Processing and confidence metrics")
    
    # Legacy fields for compatibility
    documents_found: int = Field(..., description="Number of documents found (legacy)")
    has_context: bool = Field(..., description="Whether context was used")
    confidence: float = Field(..., description="Confidence score (legacy)")
    response_time_ms: int = Field(..., description="Response time (legacy)")
    model_used: str = Field(..., description="AI model used")
    query_id: str = Field(..., description="Unique query identifier")
    rag_pipeline: Dict[str, Any] = Field(default_factory=dict, description="RAG pipeline metadata")
    
    # AI Intelligence fields (preserved)
    related_questions: List[str] = Field(default_factory=list)
    document_recommendations: List[Dict[str, Any]] = Field(default_factory=list)
    ai_intelligence_enabled: bool = Field(default=True)
    query_intelligence: Optional[Dict[str, Any]] = Field(None)
    response_optimization: Optional[Dict[str, Any]] = Field(None)
    formatting_applied: Optional[str] = Field(None)

# Context7 verified helper functions for enhanced formatting
def extract_slide_number(content: str, source: str) -> Optional[str]:
    """Extract slide number from document content or source"""
    # Try to find slide references in content
    slide_patterns = [
        r'--- Slide (\d+) ---',
        r'Slide (\d+)',
        r'sayfa (\d+)',
        r'Page (\d+)',
        r'--- (\d+) ---'
    ]
    
    for pattern in slide_patterns:
        match = re.search(pattern, content, re.IGNORECASE)
        if match:
            return f"Slide {match.group(1)}"
    
    # Try to extract from source filename
    filename_pattern = r'slide[_\s]?(\d+)|sayfa[_\s]?(\d+)|page[_\s]?(\d+)'
    match = re.search(filename_pattern, source, re.IGNORECASE)
    if match:
        slide_num = match.group(1) or match.group(2) or match.group(3)
        return f"Slide {slide_num}"
    
    return None

def calculate_relevance_percentage(score: float) -> float:
    """Convert similarity score to relevance percentage (Context7 verified)"""
    # Context7 pattern: Normalize scores to meaningful percentages
    if score >= 0.9:
        return 90.0 + (score - 0.9) * 100  # 90-100%
    elif score >= 0.7:
        return 70.0 + (score - 0.7) * 100  # 70-90%
    elif score >= 0.5:
        return 50.0 + (score - 0.5) * 100  # 50-70%
    else:
        return max(0.0, score * 100)  # 0-50%

def create_structured_sections(answer: str, learned_sources: List[Dict], doc_sources: List[Dict]) -> List[StructuredSection]:
    """Create structured sections from answer content (Context7 verified)"""
    sections = []
    
    # Check if there are learned procedures to highlight
    if learned_sources:
        learned_content = ""
        for source in learned_sources[:2]:  # Top 2 learned items
            content = source.get('content', '')[:200] + "..."
            learned_content += f"‚Ä¢ {content}\n"
        
        if learned_content:
            sections.append(StructuredSection(
                title="üí° √ñƒürenilen Prosed√ºr",
                content=learned_content.strip(),
                icon="üí°",
                priority=1
            ))
    
    # Main answer content
    main_content = answer
    if len(main_content) > 300:
        # Split into introduction and details if long
        intro = main_content[:300] + "..."
        sections.append(StructuredSection(
            title="üìã √ñzet",
            content=intro,
            icon="üìã",
            priority=2
        ))
        
        if len(main_content) > 300:
            details = main_content[300:]
            sections.append(StructuredSection(
                title="üìù Detaylar",
                content=details,
                icon="üìù",
                priority=3
            ))
    else:
        sections.append(StructuredSection(
            title="üìã A√ßƒ±klama",
            content=main_content,
            icon="üìã",
            priority=2
        ))
    
    # Additional info if multiple sources
    if len(doc_sources) > 1:
        source_info = f"Bu bilgi {len(doc_sources)} farklƒ± kaynaktan derlenmi≈ütir."
        sections.append(StructuredSection(
            title="üìö Kaynak Bilgisi",
            content=source_info,
            icon="üìö",
            priority=4
        ))
    
    return sections

class ChatQueryResponse(BaseModel):
    answer: str
    sources: List[SourceInfo] = []
    documents_found: int = 0
    has_context: bool = False
    confidence: float = 0.0
    response_time_ms: int = 0
    model_used: str = "gemini-2.5-flash-lite-preview-06-17"
    query_id: str
    rag_pipeline: Dict[str, Any] = {}

    # AI Intelligence fields (Context7 verified)
    related_questions: List[str] = []
    document_recommendations: List[Dict[str, Any]] = []
    ai_intelligence_enabled: bool = True
    # NEW: Query Intelligence & Response Optimization fields
    query_intelligence: Optional[Dict[str, Any]] = None
    response_optimization: Optional[Dict[str, Any]] = None
    formatting_applied: Optional[str] = None
    
    # ENHANCED FIELDS (Context7 verified) - Added for improved formatting
    sections: List[StructuredSection] = Field(default_factory=list, description="Structured content sections")
    metrics: Optional[ProcessingMetrics] = Field(None, description="Processing and confidence metrics")



# Core database
from app.core.db import get_session
from app.models import ConversationMessage, Conversation
from app.api.routes.conversations import get_or_create_conversation
from app.services.chat_title_service import chat_title_service



# Import authentication (Context7 verified)
from app.core.auth_middleware import get_current_user_optional_dependency
from app.models import User

# Import WebSocket service for real-time notifications (Context7 verified)
from app.services.websocket_service import websocket_service

# Phase 5.4: Import caching service (Context7 verified)
from app.services.caching_service import caching_service



# Google GenAI SDK integration (Context7 verified)
try:
    from google import genai
    from google.genai import types
    from app.core.config import settings
    from app.services.api_rotation import get_rotation_service, initialize_rotation_service
    
    # Initialize rotation service if multiple keys available
    if settings.USE_API_ROTATION and len(settings.parsed_gemini_api_keys) > 1:
        initialize_rotation_service(settings.parsed_gemini_api_keys)
        GEMINI_AVAILABLE = True
        USE_ROTATION = True
        print(f"[OK] Chat API rotation initialized with {len(settings.parsed_gemini_api_keys)} keys")
    else:
        # Create single client using API key
        client = genai.Client(api_key=settings.GEMINI_API_KEY)
        GEMINI_AVAILABLE = True
        USE_ROTATION = False
        print("[OK] Chat API single client initialized")
    
except ImportError:
    GEMINI_AVAILABLE = False
    USE_ROTATION = False
    print("Warning: google-genai not installed")

# RAG Services (Context7 verified)
from app.services.embeddings import embeddings_service
from app.services.vector_store import vector_store_service

# Context7 Hybrid Search Import
try:
    from app.services.hybrid_rag_service import hybrid_rag_service
    HYBRID_SEARCH_AVAILABLE = True
    print("‚úÖ Context7: Hybrid RAG service available")
except ImportError:
    HYBRID_SEARCH_AVAILABLE = False
    print("‚ö†Ô∏è Hybrid RAG service not available, using standard search")

router = APIRouter(tags=["chat"])

@router.post("/query", response_model=ChatQueryResponse)
async def query_documents(request: ChatQueryRequest, session: Session = Depends(get_session)):
    """
    Real RAG Q&A endpoint using Google Gemini API + Vector Search (Context7 verified)
    
    Features:
    - Document search with vector similarity
    - Multi-provider API key rotation
    - Response optimization via Gemini
    - Real-time WebSocket notifications
    """
    if not GEMINI_AVAILABLE:
        raise HTTPException(
            status_code=503,
            detail="AI service not available - google-genai package not installed"
        )
    
    question = request.question
    if not question:
        raise HTTPException(status_code=400, detail="Question is required")
    
    query_id = str(uuid.uuid4())
    start_time = time.time()
    
    # Context7 verified: Get or create conversation for persistence
    conversation = get_or_create_conversation(
        session_id=request.conversation_id or "default",
        user_id=None,  # User ID from auth when available
        session=session
    )
    
    if conversation:
        print(f"üí¨ Using conversation: {conversation.id} (title: {conversation.title})")
    else:
        print("‚ö†Ô∏è Could not create/find conversation - proceeding without persistence")
    
    # Phase 5.4: Check cache first (Context7 verified)
    cached_response = await caching_service.get_cached_response(question)
    if cached_response:
        return ChatQueryResponse(
            answer=cached_response["response"]["answer"],
            sources=[SourceInfo(**source) for source in cached_response["sources"]],
            documents_found=cached_response["response"].get("documents_found", 0),
            has_context=cached_response["response"].get("has_context", False),
            confidence=cached_response["confidence"],
            response_time_ms=cached_response["response_time_ms"],
            model_used=cached_response["response"].get("model_used", "gemini-cached"),
            query_id=query_id,
            rag_pipeline=cached_response["response"].get("rag_pipeline", {"type": "cached"}),

            related_questions=cached_response["response"].get("related_questions", []),
            document_recommendations=cached_response["response"].get("document_recommendations", []),
            ai_intelligence_enabled=True
        )
    
    # Context7 verified: Save user message to conversation
    user_message = None
    if conversation:
        try:
            user_message = ConversationMessage(
                conversation_id=str(conversation.id),
                message_type="user",
                content=question,
                message_order=conversation.message_count + 1,
                tokens_used=0,
                created_at=datetime.utcnow()
            )
            session.add(user_message)
            
            # Update conversation metadata
            conversation.message_count += 1
            conversation.last_activity = datetime.utcnow()
            conversation.updated_at = datetime.utcnow()
            
            session.commit()
            print(f"‚úÖ User message saved to conversation {conversation.id}")
        except Exception as e:
            session.rollback()
            print(f"‚ö†Ô∏è Failed to save user message: {e}")
    
    # Start normal RAG processing
    print(f"üß† STEP 1: Starting RAG query processing for: {question[:50]}...")
    print(f"üí¨ Request conversation_id: {request.conversation_id}")
    print(f"üåê CORS Debug - Origin allowed: localhost:5174")
    print(f"üìù Query: {question}")
    # NEW: Step 0.5 - INTELLIGENT QUERY PROCESSING & RESPONSE OPTIMIZATION
    # Context7 verified pattern: Graceful degradation without deleted services
    print(f"üß† STEP 0.5: Core RAG Processing (Simplified)")
        
    # Create basic analysis object for compatibility (Context7 verified fallback pattern)
    query_analysis = {
        "original_query": question,
        "query_type": "GENERAL_INQUIRY", 
        "complexity": "MEDIUM",
        "intent": "General inquiry",
        "key_entities": [],
        "context_needed": True,
        "preferred_format": "SUMMARY",
        "banking_category": None,
        "urgency_level": 3,
        "confidence_score": 0.5,
        "processing_notes": "Fallback analysis due to service error"
    }
    
    # Continue with Enhanced RAG processing (Documents + Learned Knowledge)
    try:
        # RAG Pipeline Step 1: Create multi-query embeddings for better search
        print(f"Creating embeddings for query: {question}")
        query_embeddings = await embeddings_service.create_multi_query_embedding(question)
        print(f"Generated {len(query_embeddings)} query variations")
        
        # RAG Pipeline Step 2: ENHANCED SEARCH - Documents + Learned Knowledge
        print("üîç Enhanced search in documents AND learned knowledge...")
        
        # Search in documents (existing logic)
        similar_docs = []
        if HYBRID_SEARCH_AVAILABLE:
            print("üîç Using Context7 Hybrid Search (BM25 + Semantic + Re-ranking)")
            
            # Get all documents for hybrid indexing
            all_docs = await vector_store_service.get_all_documents()
            
            if all_docs:
                # Index documents for hybrid search
                hybrid_docs = [{"content": doc.get("content", ""), "id": doc.get("id", "")} for doc in all_docs]
                
                if hybrid_rag_service.index_documents(hybrid_docs):
                    # Perform hybrid search
                    hybrid_results = hybrid_rag_service.hybrid_search(
                        query=question,
                        top_k=settings.DEFAULT_TOP_K,
                        rerank=True
                    )
                    
                    # Convert hybrid results to standard format
                    for result in hybrid_results:
                        similar_docs.append({
                            "content": result["content"],
                            "source": result["document_id"],
                            "score": result.get("cross_score", result.get("hybrid_score", 0.0)),
                            "id": result["document_id"],
                            "page": 0,
                            "search_type": result.get("search_type", "hybrid")
                        })
                    
                    print(f"‚úÖ Hybrid document search found {len(similar_docs)} results")
                else:
                    print("‚ö†Ô∏è Hybrid indexing failed, falling back to standard search")
                    similar_docs = await vector_store_service.search_similar_documents_multi_query(
                        query_embeddings=query_embeddings,
                        top_k=settings.DEFAULT_TOP_K,
                        filter_metadata=None
                    )
            else:
                print("‚ö†Ô∏è No documents found for hybrid search, using standard search")
                similar_docs = await vector_store_service.search_similar_documents_multi_query(
                    query_embeddings=query_embeddings,
                    top_k=settings.DEFAULT_TOP_K,
                    filter_metadata=None
                )
        else:
            # Fallback to standard search
            print("üîç Using standard semantic search")
            similar_docs = await vector_store_service.search_similar_documents_multi_query(
                query_embeddings=query_embeddings,
                top_k=settings.DEFAULT_TOP_K,
                filter_metadata=None
            )
        
        # RAG Pipeline Step 2.5: NEW - Search in Learned Knowledge
        learned_knowledge = []
        try:
            print("üéì STEP 2.5: Searching learned knowledge...")
            print(f"üîç Query embeddings count: {len(query_embeddings)}")
            try:
                first_dim = len(query_embeddings[0]) if query_embeddings and len(query_embeddings) > 0 else 0
            except (TypeError, IndexError):
                first_dim = 0
            print(f"üìè First embedding dimension: {first_dim}")
            
            with next(get_session()) as session:
                print("üìÅ Database session created, calling search function...")
                learned_knowledge = await search_learned_knowledge(
                    query_embeddings=query_embeddings,
                    top_k=3,  # Get top 3 learned items
                    session=session
                )
                print(f"‚úÖ Learned knowledge search returned {len(learned_knowledge)} items")
                for i, item in enumerate(learned_knowledge):
                    print(f"   üéØ {i+1}. {item.get('source', 'Unknown')}: {item.get('content', '')[:100]}...")
        except Exception as e:
            print(f"‚ùå ERROR in learned knowledge search: {str(e)}")
            import traceback
            print(f"üî• Full traceback: {traceback.format_exc()}")
            learned_knowledge = []  # Continue with empty learned knowledge
        
        # RAG Pipeline Step 3: Combine Documents + Learned Knowledge
        print(f"STEP 3: Combining {len(similar_docs)} documents + {len(learned_knowledge)} learned knowledge")
        all_sources = similar_docs + learned_knowledge
        all_sources.sort(key=lambda x: x["score"], reverse=True)
        
        # Take top results (mix of documents and learned knowledge)
        top_sources = all_sources[:settings.DEFAULT_TOP_K]
        
        print(f"Combined search results: {len(similar_docs)} documents + {len(learned_knowledge)} learned items = {len(top_sources)} total")
        
        # RAG Pipeline Step 4: Use retrieved sources if available
        if top_sources:
            # Build context from retrieved chunks
            context_parts = []
            sources = []
            
            for i, doc in enumerate(top_sources):
                # Context7 verified enhanced source info creation
                source = doc.get("source", "")
                content = doc.get("content", "")
                score = doc.get("score", 0)
                
                # Extract slide number using Context7 verified function
                slide_number = extract_slide_number(content, source)
                
                # Calculate relevance percentage using Context7 verified function
                relevance_percentage = calculate_relevance_percentage(score)
                
                # Determine content type
                content_type = "learned" if source.startswith("üí° √ñƒürenilen Bilgi") else "document"
                
                source_info = SourceInfo(
                    source=source,
                    page=slide_number or doc.get("page", 0),
                    score=round(score, 3),
                    chunk_id=doc.get("id", ""),
                    # Enhanced fields (Context7 verified)
                    slide_number=slide_number,
                    relevance_percentage=round(relevance_percentage, 1),
                    content_type=content_type
                )
                sources.append(source_info)
                
                # Format context
                context_parts.append(f"[Kaynak {i+1}] {doc.get('content', '')}")
            
            context = "\n\n".join(context_parts)
            
            # Enhanced debug output
            print(f"üîç DEBUG - Context being sent to AI:")
            print("="*50)
            for i, doc in enumerate(top_sources[:5]):  # Show first 5 for debugging
                content = doc.get("content", "")
                source = doc.get("source", "unknown")
                print(f"[Kaynak {i+1}] {content}")
            print("="*50)
            print(f"üîç DEBUG - User question: {question}")
            print("="*50)
            
            # *** ADDITIONAL DEBUG: Show the exact full content ***
            print(f"üîç FULL CONTEXT DEBUG:")
            print("-"*50)
            for i, doc in enumerate(top_sources):
                content = doc.get("content", "")
                source = doc.get("source", "unknown")
                print(f"FULL CHUNK {i+1} from {source}:")
                print(f"Content: '{content}'")
                print(f"Length: {len(content)} chars")
                print("-"*30)
            print("-"*50)
            
            # Also write to file for debugging
            with open("debug_context.txt", "w", encoding="utf-8") as f:
                f.write(f"USER QUESTION: {question}\n")
                f.write("=" * 50 + "\n")
                f.write("CONTEXT SENT TO AI:\n")
                f.write(context)
                f.write("\n" + "=" * 50 + "\n")
            
            # RAG Pipeline Step 5: Create enhanced prompt with priority for learned knowledge
            
            # Separate learned knowledge from documents
            learned_sources = [doc for doc in top_sources if doc.get("source", "").startswith("üí° √ñƒürenilen Bilgi")]
            document_sources = [doc for doc in top_sources if not doc.get("source", "").startswith("üí° √ñƒürenilen Bilgi")]
            
            # Build enhanced context with priorities
            learned_context = ""
            document_context = ""
            
            if learned_sources:
                learned_parts = []
                for i, doc in enumerate(learned_sources):
                    learned_parts.append(f"[√ñƒürenilen {i+1}] {doc.get('content', '')}")
                learned_context = "\n\n".join(learned_parts)
            
            if document_sources:
                doc_parts = []
                for i, doc in enumerate(document_sources):
                    doc_parts.append(f"[Belge {i+1}] {doc.get('content', '')}")
                document_context = "\n\n".join(doc_parts)
            
            prompt = f"""Sen bir profesyonel T√ºrk bankacƒ±lƒ±k uzmanƒ±sƒ±n. Kullanƒ±cƒ± sorularƒ±nƒ± yanƒ±tlarken √∂ƒürenilen bilgileri √ñNCELƒ∞KLE kullan, sonra belge bilgilerini kullan.

{"üéì √ñƒûRENƒ∞LEN Bƒ∞LGƒ∞LER (√ñNCELƒ∞KLƒ∞):" if learned_context else ""}
{learned_context}

{"üìÑ BELGE Bƒ∞LGƒ∞LERƒ∞:" if document_context else ""}
{document_context}

KULLANICI SORUSU: {question}

√ñNEMLƒ∞ TALƒ∞MATLAR:
1. **√ñƒûRENƒ∞LEN Bƒ∞LGƒ∞ √ñNCELƒ∞ƒûƒ∞**: Eƒüer √∂ƒürenilen bilgiler arasƒ±nda soruyla ilgili bir prosed√ºr/bilgi varsa, bunu yanƒ±tƒ±n en ba≈üƒ±nda ve net ≈üekilde belirt
2. **√ñƒûRENƒ∞LEN Bƒ∞LGƒ∞ VURGUSU**: √ñƒürenilen bilgileri "üí° **√ñƒürenilen Prosed√ºr:**" ba≈ülƒ±ƒüƒ±yla vurgula
3. **KAYNAK BELƒ∞RTME**: √ñƒürenilen bilgileri [√ñƒürenilen 1], belge bilgilerini [Belge 1] formatƒ±nda belirt
4. **TAM ENTEGRASYON**: Hem √∂ƒürenilen hem de belge bilgilerini kullanarak kapsamlƒ± yanƒ±t ver
5. **SPESƒ∞Fƒ∞K √ñNCELƒ∞K**: Kullanƒ±cƒ±nƒ±n tam sorusuna √∂ƒürenilen bilgiler cevap veriyorsa, bunu yanƒ±tƒ±n en ba≈üƒ±na koy

YANITLAMA STRATEJƒ∞Sƒ∞:
- √ñnce √∂ƒürenilen prosed√ºrleri kontrol et ve uygunsa kullan
- Sonra belge bilgileriyle destekle veya geni≈ület
- Her bilgi kaynaƒüƒ±nƒ± a√ßƒ±k√ßa belirt

CEVAP:"""

        else:
            # No relevant documents found
            print("No relevant documents found, using general knowledge")
            sources = []
            prompt = f"""Sen bir profesyonel T√ºrk bankacƒ±lƒ±k uzmanƒ±sƒ±n.

‚ö†Ô∏è DURUM: Sisteme y√ºklenmi≈ü belgeler arasƒ±nda bu soruyla doƒürudan ilgili spesifik bir belge bulunamadƒ±.

KULLANICI SORUSU: {question}

G√ñREVƒ∞N:
1. Genel bankacƒ±lƒ±k bilginle soruya kƒ±sa ve faydalƒ± bir cevap ver
2. Daha kesin bilgi i√ßin ilgili belgelerin sisteme y√ºklenmesi gerektiƒüini belirt
3. M√ºmk√ºnse genel prosed√ºrler hakkƒ±nda bilgi ver
4. T√ºrk√ße olarak yanƒ±tla

√ñNEMLƒ∞: Genel bilgi verirken, spesifik kurum politikalarƒ± i√ßin belge y√ºklenmesi gerektiƒüini hatƒ±rlat.

CEVAP:"""

        # RAG Pipeline Step 6: Generate response using Gemini 2.5 Flash-Lite Preview
        if USE_ROTATION:
            # Use rotation service for content generation
            rotation_service = get_rotation_service()
            if rotation_service:
                response_text = await rotation_service.generate_content_with_rotation(
                    contents=prompt,
                    model=settings.GEMINI_MODEL,
                    config=types.GenerateContentConfig(
                        temperature=settings.DEFAULT_TEMPERATURE,
                        max_output_tokens=1000,
                    )
                )
                if response_text is None:
                    raise Exception("All API keys exhausted - no response generated")
                
                # Create a mock response object for compatibility
                class MockResponse:
                    def __init__(self, text):
                        self.text = text
                response = MockResponse(response_text)
            else:
                raise Exception("Rotation service not available")
        else:
            # Use single client
            response = await asyncio.to_thread(
                client.models.generate_content,
                model=settings.GEMINI_MODEL,
                contents=prompt,
                config=types.GenerateContentConfig(
                    temperature=settings.DEFAULT_TEMPERATURE,
                    max_output_tokens=1000,
                )
            )
        
        response_time_ms = int((time.time() - start_time) * 1000)
        
        # Ensure response text is not None
        raw_answer = response.text if response and response.text else "√úzg√ºn√ºm, yanƒ±t olu≈üturulamadƒ±. L√ºtfen tekrar deneyin."
        
        # NEW: Step 7 - INTELLIGENT RESPONSE OPTIMIZATION & FORMATTING (TEMPORARILY DISABLED)
        # CONTEXT7 ROLLBACK: Services causing content deletion, using raw answer directly
        print(f"üéØ STEP 7: Intelligent Response Optimization... (DISABLED)")
        
        # TEMPORARILY SKIP optimization that deletes content
        # try:
        #     # Optimize the raw AI response based on query analysis
        #     optimized_response = await response_optimization_service.optimize_response(
        #         original_response=raw_answer,
        #         query_analysis=query_analysis
        #     )
        #     
        #     # Apply context-aware formatting
        #     formatted_response = await context_formatter_service.format_response(
        #         optimized_response=optimized_response,
        #         query_analysis=query_analysis,
        #         user_context={
        #             "conversation_id": request.conversation_id,
        #             "conversation_length": len(request.conversation_context or []),
        #             "session_type": "chat_api"
        #         }
        #     )
        #     
        #     # Use optimized and formatted content
        #     final_answer = formatted_response.content
        # except Exception as e:
        
        # Ensure raw_answer is valid
        raw_answer = raw_answer if raw_answer else "√úzg√ºn√ºm, yanƒ±t olu≈üturulamadƒ±. L√ºtfen tekrar deneyin."
        
        # NEW: Context7 verified Response Optimization using Gemini API
        try:
            print(f"ü§ñ STEP 7: Multi-Agent Response Optimization...")
            from app.services.response_optimizer_service import optimize_rag_response
            
            # Gemini API optimization workflow
            optimization_result = await optimize_rag_response(raw_answer)
            
            if optimization_result.optimization_applied:
                final_answer = optimization_result.optimized_response.optimized_content
                print(f"‚úÖ Response optimized successfully!")
                print(f"üìä Analysis - Clarity: {optimization_result.analysis.clarity_score}/10")
                print(f"‚ö° Optimization Score: {optimization_result.optimized_response.optimization_score}/10")
                print(f"üïí Optimization Time: {optimization_result.processing_time_ms}ms")
                
                # Store optimization metadata
                optimization_metadata = {
                    "applied": True,
                    "clarity_score": optimization_result.analysis.clarity_score,
                    "optimization_score": optimization_result.optimized_response.optimization_score,
                    "processing_time_ms": optimization_result.processing_time_ms,
                    "duplicates_removed": len(optimization_result.optimized_response.removed_duplicates),
                    "improvements_applied": len(optimization_result.optimized_response.formatting_improvements)
                }
            else:
                final_answer = raw_answer
                optimization_metadata = {
                    "applied": False,
                    "reason": "Gemini API optimization not available",
                    "processing_time_ms": optimization_result.processing_time_ms
                }
                print(f"‚ö†Ô∏è Response optimization skipped - using raw answer")
            
        except Exception as e:
            final_answer = raw_answer
            optimization_metadata = {
                "applied": False,
                "error": str(e),
                "processing_time_ms": 0
            }
            print(f"‚ö†Ô∏è Response optimization failed: {e}")
            print(f"üìù Using raw answer as fallback")
        
        optimized_response = None
        formatted_response = None
        
        print(f"üìù Final answer length: {len(final_answer)} characters")
        
        # Simplified response without AI intelligence features
        related_questions = []
        document_recommendations = []
        
        # Context7-verified: Real-time WebSocket notification (non-blocking)
        try:
            conversation_id = request.conversation_id if request.conversation_id else "default"
            await websocket_service.publish_chat_message(
                conversation_id=conversation_id,
                message_data={
                    "query_id": query_id,
                    "question": question,
                    "answer": final_answer[:200] + "..." if len(final_answer) > 200 else final_answer,
                    "documents_found": len(top_sources),
                    "response_time_ms": response_time_ms,
                    "timestamp": datetime.utcnow().isoformat()
                }
            )
        except Exception as e:
            print(f"‚ö†Ô∏è WebSocket notification error (non-critical): {e}")
            # Continue with response even if WebSocket fails

        # Phase 5.4: Cache the successful response (Context7 verified)
        response_data = {
            "answer": final_answer,
            "documents_found": len(top_sources),
            "has_context": len(top_sources) > 0,
            "model_used": settings.GEMINI_MODEL,
            "rag_pipeline": {
                "embedding_created": True,
                "documents_searched": True,
                "context_used": len(top_sources) > 0,
                "top_k": settings.DEFAULT_TOP_K
            },
            "related_questions": related_questions,
            "document_recommendations": document_recommendations
        }
        
        # Cache the response for future queries (non-blocking)
        try:
            await caching_service.cache_response(
                query=question,
                response_data=response_data,
                sources=[source for source in sources],
                confidence=0.9 if top_sources else 0.3,
                response_time_ms=response_time_ms,
                user_id=None,  # Can be enhanced to include user context
                ttl_hours=24
            )
        except Exception as e:
            print(f"‚ö†Ô∏è Caching error (non-critical): {e}")

        # Context7 verified: Save assistant message to conversation
        if conversation:
            try:
                assistant_message = ConversationMessage(
                    conversation_id=str(conversation.id),
                    message_type="assistant",
                    content=final_answer,
                    message_order=conversation.message_count + 1,
                    tokens_used=len(final_answer.split()),  # Rough token estimation
                    model_used=settings.GEMINI_MODEL,
                    response_time_ms=response_time_ms,
                    confidence_score=0.9 if top_sources else 0.3,
                    sources_used=[source.source for source in sources] if sources else [],
                    created_at=datetime.utcnow()
                )
                session.add(assistant_message)
                
                # Update conversation metadata
                conversation.message_count += 1
                conversation.last_activity = datetime.utcnow()
                conversation.updated_at = datetime.utcnow()
                
                session.commit()
                print(f"‚úÖ Assistant message saved to conversation {conversation.id}")
                
                # Context7 verified: Auto-generate intelligent title if needed
                if conversation.message_count == 2 and conversation.title.startswith("Chat "):
                    try:
                        print(f"üéØ Generating intelligent title for conversation {conversation.id}")
                        intelligent_title = await chat_title_service.generate_title(
                            first_user_message=question,
                            first_ai_response=final_answer
                        )
                        
                        if intelligent_title and intelligent_title != conversation.title:
                            conversation.title = intelligent_title
                            session.commit()
                            print(f"‚úÖ Updated conversation title to: '{intelligent_title}'")
                    except Exception as title_error:
                        print(f"‚ö†Ô∏è Title generation failed: {title_error}")
                        
            except Exception as e:
                session.rollback()
                print(f"‚ö†Ô∏è Failed to save assistant message: {e}")

        return ChatQueryResponse(
            answer=final_answer,
            sources=sources,
            documents_found=len(top_sources),
            has_context=len(top_sources) > 0,
            confidence=0.9 if top_sources else 0.3,  # Higher confidence with sources
            response_time_ms=response_time_ms,
            model_used=settings.GEMINI_MODEL,
            query_id=query_id,
            rag_pipeline={
                "embedding_created": True,
                "documents_searched": True,
                "context_used": len(top_sources) > 0,
                "top_k": settings.DEFAULT_TOP_K,
                "intelligent_processing": True
            },
            # AI Intelligence fields (Context7 verified)
            related_questions=related_questions,
            document_recommendations=document_recommendations,
            ai_intelligence_enabled=True,
            # NEW: Intelligent processing fields (Context7 verified safe access)
            query_intelligence={
                "type": query_analysis.get("query_type") if query_analysis else None,
                "complexity": query_analysis.get("complexity") if query_analysis else None,
                "preferred_format": query_analysis.get("preferred_format") if query_analysis else None,
                "urgency": query_analysis.get("urgency_level") if query_analysis else None,
                "confidence": query_analysis.get("confidence_score") if query_analysis else None,
                "banking_category": query_analysis.get("banking_category") if query_analysis else None,
                "entities": query_analysis.get("key_entities", []) if query_analysis else []
            } if query_analysis else None,
            response_optimization=optimization_metadata if 'optimization_metadata' in locals() else {
                "applied": False,
                "reason": "Optimization not attempted",
                "processing_time_ms": 0
            },
            formatting_applied=None,
            
            # ENHANCED FIELDS (Context7 verified) - New structured formatting
            sections=create_structured_sections(
                answer=final_answer,
                learned_sources=[doc for doc in top_sources if doc.get("source", "").startswith("üí° √ñƒürenilen Bilgi")],
                doc_sources=[doc for doc in top_sources if not doc.get("source", "").startswith("üí° √ñƒürenilen Bilgi")]
            ),
            metrics=ProcessingMetrics(
                response_time_ms=response_time_ms,
                confidence_score=0.9 if top_sources else 0.3,
                documents_found=len([doc for doc in top_sources if not doc.get("source", "").startswith("üí° √ñƒürenilen Bilgi")]),
                learned_knowledge_found=len([doc for doc in top_sources if doc.get("source", "").startswith("üí° √ñƒürenilen Bilgi")]),
                embedding_dimension=len(query_embeddings[0]) if query_embeddings and len(query_embeddings) > 0 else 1536,
                search_strategy="hybrid" if HYBRID_SEARCH_AVAILABLE and all_docs else "semantic"
            )
        )
        
    except Exception as e:
        response_time_ms = int((time.time() - start_time) * 1000)
        print(f"‚ùå RAG Pipeline Error: {str(e)}")
        # Context7 verified: Return structured error response
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail={
                "error": "RAG pipeline error",
                "message": str(e),
                "response_time_ms": response_time_ms,
                "query_id": query_id
            }
        )

@router.get("/health")
async def chat_health_check():
    """Real chat service health check with RAG pipeline status (Context7 verified)"""
    status = {
        "service": "chat",
        "gemini_available": GEMINI_AVAILABLE,
        "model": settings.GEMINI_MODEL
    }
    
    if GEMINI_AVAILABLE:
        try:
            # Test Gemini API with a simple query (Context7 verified pattern)
            test_response = await asyncio.to_thread(
                client.models.generate_content,
                model=settings.GEMINI_MODEL,
                contents="Test",
                config=types.GenerateContentConfig(max_output_tokens=10)
            )
            status["gemini_api"] = "healthy"
            status["test_response"] = test_response.text[:50] + "..."
        except Exception as e:
            status["gemini_api"] = f"error: {str(e)}"
    
    # Test RAG pipeline components
    try:
        # Test embeddings service
        status["embeddings_service"] = "available"
        status["embeddings_model"] = embeddings_service.model
        status["embeddings_dimension"] = embeddings_service.dimensions
    except Exception as e:
        status["embeddings_service"] = f"error: {str(e)}"
    
    try:
        # Test vector store
        vector_stats = await vector_store_service.get_index_stats()
        status["vector_store"] = "available"
        status["vector_stats"] = vector_stats
    except Exception as e:
        status["vector_store"] = f"error: {str(e)}"
    
    return status

@router.get("/rotation-status")
async def get_rotation_status():
    """Get API rotation service status"""
    if not USE_ROTATION:
        return {
            "rotation_enabled": False,
            "message": "API rotation not enabled"
        }
    
    rotation_service = get_rotation_service()
    if not rotation_service:
        return {
            "rotation_enabled": True,
            "status": "error",
            "message": "Rotation service not initialized"
        }
    
    status_report = rotation_service.get_status_report()
    current_key_info = rotation_service.get_current_key_info()
    
    return {
        "rotation_enabled": True,
        "status": "active",
        "current_key": current_key_info,
        "summary": status_report,
        "message": f"Using key {status_report.get('current_key', 'unknown') if status_report else 'unknown'}/{status_report.get('total_keys', 'unknown') if status_report else 'unknown'}"
    } 